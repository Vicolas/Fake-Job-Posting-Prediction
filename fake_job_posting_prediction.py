# -*- coding: utf-8 -*-
"""Fake Job Posting Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JjIpU0GRwZTcAUXxqL9MpS9lxpvIDUm-
"""

#install packages: Word cloud is used for visualising frequent words 
#in a text where the size of the words represents their frequency

!pip install wordcloud

#install packages: spaCy is used to build information extraction or natural language understanding systems

!pip install -U spacy

# import libraries

import re
import string
import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.base import TransformerMixin
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay,classification_report, confusion_matrix
from wordcloud import WordCloud
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

#importing the dataset

df = pd.read_csv('/gdrive/MyDrive/Datasets/fake_job_postings.csv')

"""# DATA INFORMATION"""

#checking the first 5 columns of the dataset

df.head()

#checking the number of rows and columns of the dataset

df.shape

#checking the total of columns and their values, both null and true

df.isnull().sum()

#delete unnecessary columns

columns=['job_id', 'telecommuting', 'has_company_logo', 'has_questions', 'salary_range', 'employment_type']
for col in columns:
    del df[col]

#checking the first 5 columns of the dataset after filling NaN values with blank

df.head()

#fill NaN spaces with blank

df.fillna(' ', inplace=True)

#understanding the number of fraudulent and non fraudulent job posting in our dataset

df.groupby('fraudulent')['fraudulent'].count()

#compare the number of fraudulent and non fraudulent job postings
#using seaborn to visualize

plt.figure(figsize=(15,5))
sns.countplot(y='fraudulent', data=df)
plt.show()

"""# DATA VISUALIZATION

"""

# visualisiing no. of jobs based on experience. First, I create a dictionary function that creats key value pairs
# with the categories found in the experience column and the count, second line deletes the blanks in the column
exp = dict(df.required_experience.value_counts())
del exp[' ']
exp

plt.figure(figsize=(10,5))
plt.title('No. of jobs with experience', size=20)
plt.bar(exp.keys(),exp.values())
plt.ylabel('Num. of Jobs', size=10)
plt.xlabel('Experience', size=10)
plt.xticks(rotation=20)
plt.show()

#Visualize job postings by countries
#first, I create an new column named country in order to make the visualization broad and clear as the location column provided has country,state and city listed

def split(location):
    l = location.split(',')
    return l[0]
df['country'] = df.location.apply(split)

df.head()

#visualizing the number of job postings for the top 10 countries in the dataset
#creating a dictionary of key value pairs for the visuaization of 10 countries

countr = dict(df.country.value_counts()[:10])
del countr[' ']
countr

#visualizing the country with the highest number of job posting

plt.figure(figsize=(10,5))
plt.title('Country-wise Job posting', size=20)
plt.bar(countr.keys(),countr.values())
plt.ylabel('Num. of Jobs', size=10)
plt.xlabel('Countries', size=10)
plt.xticks(rotation=20)
plt.show()

#visualizing the required education level and number of job posting for each level
#creating a dictionary of key value pairs for visuaizing the top 7 job positngs in this category

edu = dict(df.required_education.value_counts()[:7])
del edu[' ']
edu

#visualizing the top educational requirement

plt.figure(figsize=(10,5))
plt.title('Job Posting Based On Education', size=20)
plt.bar(edu.keys(),edu.values())
plt.ylabel('Num. of Jobs', size=10)
plt.xlabel('Education', size=10)
plt.xticks(rotation=20)
plt.show()

#checking for the commonly used job titles when the job title is not fraudulent

print(df[df.fraudulent==0].title.value_counts()[:10])

#checking for the commonly used job titles when the job title is fraudulent

print(df[df.fraudulent==1].title.value_counts()[:10])

#checking for the industry that has the highest number of fake job postings.

print(df[df.fraudulent==0].industry.value_counts()[:10])

#visualizing the industries with the highest number of fake jobs for the top 10 industries in the dataset
#creating a dictionary of key value pairs for the visuaization of 10 industries

ind = dict(df.industry.value_counts()[:10])
del ind[' ']
ind

#visualizing the industry that has the highest number of fake job postings.

plt.figure(figsize=(10,5))
plt.title('Industry With The Highest Number Of Fake Jobs', size=20)
plt.bar(ind.keys(),ind.values())
plt.ylabel('Num. of Jobs', size=10)
plt.xlabel('Industry', size=10)
plt.xticks(rotation=20)
plt.show()

#combine some selected columns in the dataset into 1 single column as text and the rest will be deleted
# 1 column is a text column and the 2nd column will state if the job was fraudulent or not

df['text']=df['title']+' '+df['location']+' '+df['company_profile']+' '+df['description']+' '+df['requirements']+' '+df['benefits']
del df['title']
del df['location']
del df['department']
del df['company_profile']
del df['description']
del df['requirements']
del df['benefits']
del df['required_experience']
del df['required_education']
del df['industry']
del df['function']
del df['country']

df.head()

#creating wordclouds for fraudulent and non fraudulent jobs

fraudjobs_text = df[df.fraudulent==1].text
realjobs_text = df[df.fraudulent==0].text

#fraudulent jobs word cloud
STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS
plt.figure(figsize = (16,14))
wc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(" ".join(fraudjobs_text)))
plt.imshow(wc,interpolation = 'bilinear')

#real jobs wordcloud
plt.figure(figsize = (16,14))
wc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(" ".join(realjobs_text)))
plt.imshow(wc,interpolation = 'bilinear')

#install packages to work with English words in natural language processing 

!pip install spacy && python -m spacy download en

#preprocess the data
#creating a list of punctuation marks
punctuations = string.punctuation

#creating a list of stopwords
nlp = spacy.load('en_core_web_sm')
stop_words = spacy.lang.en.stop_words.STOP_WORDS

#load English tokenizer, parser, NER and word vectors
parcer = English()

#creating our tokenizer function
def spacy_tokenizer(sentence):
    #creating our token object, which is used to create documents with linguistic annotations.
    mytokens = parser(sentence)

    #lemmatizing each token and converting each token into lowercase
    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != "-PRON-" else word.lower_ for word in mytokens ]

    #removing stop words
    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]

    #return a preprocessed list of tokens
    return mytokens

#custom transformer using spaCy
class predictors(TransformerMixin):
    def transform(self, X, **transform_params):

        #cleaning Text
        return [clean_text(text) for text in X]

    def fit(self, X, y=None, **fit_params):
        return self

    def get_params(self, deep=True):
        return {}    

#basic function to clean the text
def clean_text(text):
    #removing spaces and converting text into lowercase
    return text.strip().lower()

#cleaning the text of the dataset

df['text'] = df['text'].apply(clean_text)

#using Tfidfvectorizer: a count vectorizer that gives equal advantage to all the words and 
#and converts the words to column for each document and shows how important the word is, it is used in transforming text into vectors.

cv = TfidfVectorizer(max_features = 100)
x = cv.fit_transform(df['text'])
df1 = pd.DataFrame(x.toarray(), columns = cv.get_feature_names_out())
df.drop(['text'], axis=1, inplace=True)
main_df = pd.concat([df1,df], axis=1)

#words present in our dataset and their frequencies
main_df.head()

#testing and trimming the dataset fraudulent column has been moved to variable y
Y = main_df.iloc[:,-1]
X = main_df.iloc[:,:-1]

#splitting dataset into 70% for trimming and 30% for testing
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3)

#Train-test shape
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# importing randomforest

from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(n_jobs=3, oob_score=True, n_estimators=100, criterion='entropy')
model = rfc.fit(X_train,y_train)

print(X_test)

pred = rfc.predict(X_test)
score = accuracy_score(y_test, pred)
score

# creating a confussion matrix
print('Classification Report\n')
print(classification_report(y_test, pred))
print('Confusion Matrix\n')
print(confusion_matrix(y_test, pred))